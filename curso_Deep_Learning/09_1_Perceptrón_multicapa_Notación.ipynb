{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBQzjfXaXAUywsNyxS1dz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/ai/blob/main/curso_Deep_Learning/09_1_Perceptr%C3%B3n_multicapa_Notaci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptrón Multicapa. Notación\n",
        "\n",
        "<img src=\"https://github.com/financieras/ai/blob/main/curso_Deep_Learning/img/perceptron_multicapa.png?raw=1\" alt=\"perceptrón multicapa\" width=\"800\"/>\n"
      ],
      "metadata": {
        "id": "CFtu8w-OXMNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función de agregación $z$ y función de activación $a$\n",
        "Las capas se denotan con la letra $l$:\n",
        "\n",
        "0. $l=0$ es la \"Input Layer\", la capa de entrada\n",
        "1. $l=1$ es la \"Hidden Layer 1\", la primera capa oculta\n",
        "2. $l=2$ es la \"Hidden Layer 2\", la segunda capa oculta\n",
        "3. $l=3$ es la \"Output Layer\", la capa de salida\n",
        "\n",
        "La salida de cada neurona $a$ viene dada por la aplicación de la función de activación a la función de agregación $z$. Esto es: $a=\\phi(z)$\n",
        "\n",
        "Las entradas $x_1$ y $x_2$ se pueden considerar como neuronas de valor fijo y se denotan como $a_1^{[0]}$ y $a_2^{[0]}$ ya que pertenecen a la capa $l=0$ y para usar una nomenclatura igual a la de las demás capas.\n",
        "\n",
        "\n",
        "PRIMERA CAPA OCULTA\n",
        "1. Primera neurona de la primera capa oculta:\n",
        "    - Función de agregación $z_1^{[1]} = w_{11}^{[1]} x_1 + w_{21}^{[1]} x_2 + b_1^{[1]} = w_{11}^{[1]} a_1^{[0]} + w_{21}^{[1]} a_2^{[0]} + b_1^{[1]}$\n",
        "    - Salida de la neurona $a_1^{[1]} = \\phi(z_1^{[1]})$\n",
        "2. Segunda neurona de la primera capa oculta:\n",
        "    - Función de agregación $z_2^{[1]} = w_{12}^{[1]} a_1^{[0]} + w_{22}^{[1]} a_2^{[0]} + b_2^{[1]}$\n",
        "    - Salida de la neurona $a_2^{[1]} = \\phi(z_2^{[1]})$\n",
        "3. Tercera neurona de la primera capa oculta:\n",
        "    - Función de agregación $z_3^{[1]} = w_{13}^{[1]} a_1^{[0]} + w_{23}^{[1]} a_2^{[0]} + b_3^{[1]}$\n",
        "    - Salida de la neurona $a_3^{[1]} = \\phi(z_3^{[1]})$\n",
        "\n",
        "SEGUNDA CAPA OCULTA\n",
        "1. Primera neurona de la segunda capa oculta:\n",
        "    - Función de agregación $z_1^{[2]} = w_{11}^{[2]} a_1^{[1]} + w_{21}^{[2]} a_2^{[1]} + w_{31}^{[2]} a_3^{[1]} + b_1^{[2]}$\n",
        "    - Salida de la neurona $a_1^{[2]} = \\phi(z_1^{[2]})$\n",
        "2. Segunda neurona de la segunda capa oculta:\n",
        "    - Función de agregación $z_2^{[2]} = w_{12}^{[2]} a_1^{[1]} + w_{22}^{[2]} a_2^{[1]} + w_{32}^{[2]} a_3^{[1]} + b_2^{[2]}$\n",
        "    - Salida de la neurona $a_2^{[2]} = \\phi(z_2^{[2]})$\n",
        "3. Tercera neurona de la segunda capa oculta:\n",
        "    - Función de agregación $z_3^{[2]} = w_{13}^{[2]} a_1^{[1]} + w_{23}^{[2]} a_2^{[1]} + w_{33}^{[2]} a_3^{[1]} + b_3^{[2]}$\n",
        "    - Salida de la neurona $a_3^{[2]} = \\phi(z_3^{[2]})$"
      ],
      "metadata": {
        "id": "B5x8ZgxKXSd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Ae9J1vKb3od"
      }
    }
  ]
}